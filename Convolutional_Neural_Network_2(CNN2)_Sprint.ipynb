{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4MEFn9hhgaD5z7MiCQVKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ismael-rtellez/CNN2_Series_Assignment/blob/main/Convolutional_Neural_Network_2(CNN2)_Sprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN2 Series Assignment: SimpleConv2d"
      ],
      "metadata": {
        "id": "O0jbXQSVfMGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 1】Creating a 2-D convolutional layer"
      ],
      "metadata": {
        "id": "Hqdjx7fLisEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import L\n",
        "import numpy as np\n",
        "\n",
        "# Problem 1: Creating a 2D convolutional layer\n",
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kh, self.kw = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.lr = learning_rate\n",
        "\n",
        "\n",
        "        # Xavier initialization\n",
        "        scale  = np.sqrt(1. / (in_channels * self.kh * self.kw))\n",
        "        self.W = np.random.randn(out_channels, in_channels, self.kh, self.kw) * scale\n",
        "        self.B = np.zeros(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h= (H + 2 * self.padding - self.kh) // self.stride + 1\n",
        "        out_w= (W + 2 * self.padding - self.kw) // self.stride + 1\n",
        "        self.out_shape = (N, self.out_channels, out_h, out_w)\n",
        "\n",
        "        if self.padding > 0:\n",
        "            x = np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "        self.x_padded = x\n",
        "        out = np.zeros((N, self.out_channels, out_h, out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for oc in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + self.kh\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + self.kw\n",
        "                        region = x[n, :, h_start:h_end, w_start:w_end]\n",
        "                        out[n, oc, i, j] = np.sum(region * self.W[oc]) + self.B[oc]\n",
        "        return out\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        N, C, H, W = self.x.shape\n",
        "        dx = np.zeros_like(self.x_padded, dtype=np.float32)\n",
        "        dW = np.zeros_like(self.W, dtype=np.float32)\n",
        "        dB = np.zeros_like(self.B, dtype=np.float32)\n",
        "\n",
        "        _, _, out_h, out_w = d_out.shape\n",
        "\n",
        "        for n in range(N):\n",
        "            for oc in range(self.out_channels):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + self.kh\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + self.kw\n",
        "                        region = self.x_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        dW[oc] += region * d_out[n, oc, i, j]\n",
        "                        dB[oc] += d_out[n, oc, i, j]\n",
        "                        dx[n, :, h_start:h_end, w_start:w_end] += self.W[oc] * d_out[n, oc, i, j]\n",
        "\n",
        "        # removing padding if added\n",
        "        if self.padding > 0:\n",
        "            dx = dx[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "\n",
        "        # Updating weights\n",
        "        self.W -= self.lr * dW\n",
        "        self.B -= self.lr * dB\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "P0slZG4K7Ju5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 2】Experiments with 2D convolutional layers on small arrays"
      ],
      "metadata": {
        "id": "bvxy4E_2jDvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: Experiments with 2D convolutional layerson small arrays\n",
        "\n",
        "# Input data when flowing CNN2 forwards (1, 1, 4, 4)\n",
        "x = np.array([[[[1, 2, 3, 4],\n",
        "                [5, 6, 7, 8],\n",
        "                [9, 10, 11, 12],\n",
        "                [13, 14, 15, 16]]]])\n",
        "\n",
        "# Manually setting filters\n",
        "w = np.array([\n",
        "    [[[0, 0, 0], [0, 1, 0], [0, -1, 0]]],\n",
        "    [[[0, 0, 0], [0, -1, 1], [0, 0, 0]]]\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "b = np.array([0, 0], dtype=np.float32)\n",
        "\n",
        "# Conv2d with 1 input channel, 2 outputs channels, kernel 3x3\n",
        "conv = Conv2d(in_channels=1, out_channels=2, kernel_size=(3, 3), stride=1, padding=0)\n",
        "conv.W = w.copy()\n",
        "conv.B = b.copy()\n",
        "\n",
        "# forward pass\n",
        "out = conv.forward(x)\n",
        "print(\"Forward Output: \\n\", out)\n",
        "print(\"\\n\")\n",
        "\n",
        "# backward test\n",
        "d_out = np.array([[[[-4, -4], [-4, -4]],\n",
        "                   [[1, -7], [1, -11]]]], dtype=np.float32)\n",
        "dx = conv.backward(d_out)\n",
        "print(\"Backward Output (dx): \\n\", dx)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwU_z-wfiUJ9",
        "outputId": "4ba15b9a-f508-46a6-b3c7-e4cb97a6ea31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Output: \n",
            " [[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n",
            "\n",
            "\n",
            "Backward Output (dx): \n",
            " [[[[  0.   0.   0.   0.]\n",
            "   [  0.  -5.   4.  -7.]\n",
            "   [  0.  -1.  12. -11.]\n",
            "   [  0.   4.   4.   0.]]]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 3】Output size after 2-dimensional convolution"
      ],
      "metadata": {
        "id": "ZaqamZ6OjAPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 3: Output size after 2-dimensional convolution\n",
        "def conv2d_output_size(H_in, W_in, kernel_size, stride=1, padding=0):\n",
        "    kh, kw = kernel_size\n",
        "    H_out= (H_in + 2 * padding - kh) // stride + 1\n",
        "    W_out= (W_in + 2 * padding - kw) // stride + 1\n",
        "    return H_out, W_out"
      ],
      "metadata": {
        "id": "RCpkV0Y8ieoO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 4】Creation of maximum pooling layer"
      ],
      "metadata": {
        "id": "UF60kTQ2i7B-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4: Creating a max pooling layer\n",
        "class MaxPool2D:\n",
        "    def __init__(self, pool_size=(2, 2), stride=1):\n",
        "        self.ph, self.pw = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h= (H - self.ph) // self.stride + 1\n",
        "        out_w= (W - self.pw) // self.stride + 1\n",
        "        self.arg_max = np.zeros((N, C, out_h, out_w), dtype=np.int32)\n",
        "\n",
        "        out = np.zeros((N, C, out_h, out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        window = x[n, c, h_start:h_start+self.ph, w_start:w_start+self.pw]\n",
        "                        out[n, c, i, j] = np.max(window)\n",
        "                        self.arg_max[n, c, i, j] = np.argmax(window)\n",
        "        return out\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        N, C, H, W = self.x.shape\n",
        "        out_h, out_w = d_out.shape[2:]\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        index = int(self.arg_max[n, c, i, j])\n",
        "                        h_index = h_start + index // self.pw\n",
        "                        w_index = w_start + index % self.pw\n",
        "                        dx[n, c, h_index, w_index] += d_out[n, c, i, j]\n",
        "        return dx"
      ],
      "metadata": {
        "id": "eWBIuFI0icdw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 5】(Advance task) Creating average pooling"
      ],
      "metadata": {
        "id": "PXSJ2EX4irM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 5: (Advance task) Creating average pooling\n",
        "class AveragePool2D:\n",
        "    def __init__(self, pool_size=(2, 2), stride=2):\n",
        "        self.ph, self.pw = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        N, C, H, W = x.shape\n",
        "        out_h= (H - self.ph) // self.stride + 1\n",
        "        out_w= (W - self.pw) // self.stride + 1\n",
        "\n",
        "        out = np.zeros((N, C, out_h, out_w))\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        window = x[n, c, h_start:h_start+self.ph, w_start:w_start+self.pw]\n",
        "                        out[n, c, i, j] = np.mean(window)\n",
        "        return out\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        N, C, H, W = self.x.shape\n",
        "        out_h, out_w = d_out.shape[2:]\n",
        "        dx = np.zeros_like(self.x)\n",
        "\n",
        "        for n in range(N):\n",
        "            for c in range(C):\n",
        "                for i in range(out_h):\n",
        "                    for j in range(out_w):\n",
        "                        h_start = i * self.stride\n",
        "                        w_start = j * self.stride\n",
        "                        dx[n, c, h_start:h_start+self.ph, w_start:w_start+self.pw] += d_out[n, c, i, j] / (self.ph * self.pw)\n",
        "        return dx"
      ],
      "metadata": {
        "id": "siC9-2NSiZU-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###【Problem 6】Smoothing"
      ],
      "metadata": {
        "id": "gNdPJeD1io1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 6: Smoothing\n",
        "class Flatten:\n",
        "    def forward(self, x):\n",
        "        self.orig_shape = x.shape\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        return d_out.reshape(self.orig_shape)"
      ],
      "metadata": {
        "id": "j98VlZ6miWEe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 7】Learning and estimation"
      ],
      "metadata": {
        "id": "_fangM1piErw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Problem 7: Learning and estimation\n",
        "class ReLu:\n",
        "    def forward(self, x):\n",
        "        self.mask = (x > 0)\n",
        "        return x * self.mask\n",
        "    def backward(self, d_out):\n",
        "        return d_out * self.mask\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, in_features, out_features, lr=0.01):\n",
        "        scale  = np.sqrt(1. / (in_features))\n",
        "        self.W = np.random.randn(in_features, out_features) * scale\n",
        "        self.B = np.zeros(out_features)\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(x, self.W) + self.B\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        dW = np.dot(self.x.T, d_out)\n",
        "        dB = np.sum(d_out, axis=0)\n",
        "        dx = np.dot(d_out, self.W.T)\n",
        "        self.W -= self.lr * dW\n",
        "        self.B -= self.lr * dB\n",
        "        return dx\n",
        "\n",
        "class SoftmaxCrossEntropy:\n",
        "    def forward(self, x, y):\n",
        "        self.y = y\n",
        "        self.y_pred = self._softmax(x)\n",
        "        return self._cross_entropy(self.y_pred, y)\n",
        "\n",
        "    def backward(self):\n",
        "        return (self.y_pred - self.y) / self.y.shape[0]\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        x = x - np.max(x, axis=1, keepdims=True)\n",
        "        return  np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "    def _cross_entropy(self, y_pred, y_true):\n",
        "        return -np.sum(y_true * np.log(y_pred + 1e-7)) / y_true.shape[0]\n",
        "\n",
        "# Preprocess MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train[:1000].astype(np.float32) / 255.0\n",
        "x_test = x_test[:200].astype(np.float32) / 255.0\n",
        "y_train = to_categorical(y_train[:1000], 10)\n",
        "y_test = to_categorical(y_test[:200], 10)\n",
        "\n",
        "x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "x_test = x_test.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Define simple CNN\n",
        "class Scratch2dCNNClassifier:\n",
        "    def __init__(self):\n",
        "        self.conv = Conv2d(1, 8, (3, 3), stride=1, padding=1)\n",
        "        self.relu1 = ReLu()\n",
        "        self.pool = MaxPool2D(pool_size=(2, 2), stride=2)\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = Dense(14*14*8, 64)\n",
        "        self.relu2 = ReLu()\n",
        "        self.fc2 = Dense(64, 10)\n",
        "        self.loss_fn = SoftmaxCrossEntropy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv.forward(x)\n",
        "        x = self.relu1.forward(x)\n",
        "        x = self.pool.forward(x)\n",
        "        x = self.flatten.forward(x)\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu2.forward(x)\n",
        "        x = self.fc2.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        d_out = self.fc2.backward(d_out)\n",
        "        d_out = self.relu2.backward(d_out)\n",
        "        d_out = self.fc1.backward(d_out)\n",
        "        d_out = self.flatten.backward(d_out)\n",
        "        d_out = self.pool.backward(d_out)\n",
        "        d_out = self.relu1.backward(d_out)\n",
        "        d_out = self.conv.backward(d_out)\n",
        "\n",
        "    def train(self, x, y):\n",
        "        out = self.forward(x)\n",
        "        loss = self.loss_fn.forward(out, y)\n",
        "        d_out = self.loss_fn.backward()\n",
        "        self.backward(d_out)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = self.forward(x)\n",
        "        return np.argmax(out, axis=1)\n",
        "\n",
        "\n",
        "# Training\n",
        "model = Scratch2dCNNClassifier()\n",
        "epochs = 3\n",
        "batch_size = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_sum = 0\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "        loss = model.train(x_batch, y_batch)\n",
        "        loss_sum += loss\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss_sum:.4f}\")\n",
        "\n",
        "# Accuracy\n",
        "preds = model.predict(x_test)\n",
        "true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(preds == true)\n",
        "print(\"Test Accuracy: \", accuracy)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baoPu0O6gHPQ",
        "outputId": "ef37b31d-0f52-4f1b-ac23-d628c162c2e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1, Loss: 22.6792\n",
            "Epoch 2, Loss: 22.0936\n",
            "Epoch 3, Loss: 21.5214\n",
            "Test Accuracy:  0.41\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 8】(Advance assignment) LeNet"
      ],
      "metadata": {
        "id": "8M6jyKxSjF6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 8: (Advanced Assignment) LeNet\n",
        "class LeNet:\n",
        "    def __init__(self):\n",
        "        self.conv1 = Conv2d(1, 6, (5, 5), stride=1, padding=0)\n",
        "        self.relu1 = ReLu()\n",
        "        self.pool1 = MaxPool2D(pool_size=(2, 2), stride=2)\n",
        "        self.conv2 = Conv2d(6, 16, (5, 5), stride=1, padding=0)\n",
        "        self.relu2 = ReLu()\n",
        "        self.pool2 = MaxPool2D(pool_size=(2, 2), stride=2)\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = Dense(16*4*4, 120)\n",
        "        self.relu3 = ReLu()\n",
        "        self.fc2 = Dense(120, 84)\n",
        "        self.relu4 = ReLu()\n",
        "        self.fc3 = Dense(84, 10)\n",
        "        self.loss_fn = SoftmaxCrossEntropy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1.forward(x)\n",
        "        x = self.relu1.forward(x)\n",
        "        x = self.pool1.forward(x)\n",
        "        x = self.conv2.forward(x)\n",
        "        x = self.relu2.forward(x)\n",
        "        x = self.pool2.forward(x)\n",
        "        x = self.flatten.forward(x)\n",
        "        x = self.fc1.forward(x)\n",
        "        x = self.relu3.forward(x)\n",
        "        x = self.fc2.forward(x)\n",
        "        x = self.relu4.forward(x)\n",
        "        x = self.fc3.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        d_out = self.fc3.backward(d_out)\n",
        "        d_out = self.relu4.backward(d_out)\n",
        "        d_out = self.fc2.backward(d_out)\n",
        "        d_out = self.relu3.backward(d_out)\n",
        "        d_out = self.fc1.backward(d_out)\n",
        "        d_out = self.flatten.backward(d_out)\n",
        "        d_out = self.pool2.backward(d_out)\n",
        "        d_out = self.relu2.backward(d_out)\n",
        "        d_out = self.conv2.backward(d_out)\n",
        "        d_out = self.pool1.backward(d_out)\n",
        "        d_out = self.relu1.backward(d_out)\n",
        "        d_out = self.conv1.backward(d_out)\n",
        "\n",
        "    def train(self, x, y):\n",
        "        out = self.forward(x)\n",
        "        loss = self.loss_fn.forward(out, y)\n",
        "        d_out = self.loss_fn.backward()\n",
        "        self.backward(d_out)\n",
        "        return loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = self.forward(x)\n",
        "        return np.argmax(out, axis=1)\n",
        "\n",
        "# Training Lenet\n",
        "lenet = LeNet()\n",
        "epochs = 3\n",
        "batch_size = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_sum = 0\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch = x_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "        loss = lenet.train(x_batch, y_batch)\n",
        "        loss_sum += loss\n",
        "    print(f\"[LeNet] Epoch {epoch+1}, Loss: {loss_sum:.4f}\")\n",
        "\n",
        "# Accuracy\n",
        "preds_ln = lenet.predict(x_test)\n",
        "true_ln = np.argmax(y_test, axis=1)\n",
        "acc = np.mean(preds_ln == true_ln)\n",
        "print(\"[LeNet] Test Accuracy: \", acc)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKqq9DYxXEUD",
        "outputId": "e73375c5-aa74-4dc5-bec4-40de3932ab96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LeNet] Epoch 1, Loss: 23.0647\n",
            "[LeNet] Epoch 2, Loss: 22.7274\n",
            "[LeNet] Epoch 3, Loss: 22.4579\n",
            "[LeNet] Test Accuracy:  0.235\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###【Problem 9】(Advance Assignment) Survey of famous image recognition models"
      ],
      "metadata": {
        "id": "NDUQnRAYIa-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AlexNet (2012)**\n",
        "\n",
        "AlexNet can be considered as one of the first deep convolutional neural networks that has achieved significant breakthrough in the domain of the computer vision and contributed to making deep learning popular. It has five convolutional layers then three fully connected layers. AlexNet had some of the following innovations. First, it replaced traditional sigmoid or tanh functions with ReLu (Rectifier Linear Unit) and that allowed it to speed up the trainnig process and bring better results. It also integrated dropout, regularization to minimize overfitting. It was trained by GPU on with data parallelism, and larger models could be trained. It also used Local Response Normalization (LRN) technique, but this is seldom applied in the contemporary architectures. The input of AlexNet is RGB images of dimension 224x224\n",
        "\n",
        "**VGG16 (2014)**\n",
        "\n",
        "VGG16 put forward deeper architecture involving a network than the AlexNet, entailing 13 convolutional layers and the¿ree fully  conected layers- which total up to 16 learnable layers. The main contribution of the VGG16 training process is that the small size (3*3) convolutional filters are used across the network whitout variation. This strategy proved than an aggregate or series of small filters is capable of a higher performance than 7x7 filters. The network has a plain and homogeneous structure  an this helps the creation as well as the expansion of the network. To bring about spatial dimensionality reduction, the insertion of Max pooling layers at the end of some convolutional blocks is done. Though VGG16 met better accuracy and depth unlike AlexNet, it is also slow and consumes more memory because it has more paarameters than AlexNet."
      ],
      "metadata": {
        "id": "YwDTcvGdIqcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【Problem 10】Calculation of output size and number of parameters"
      ],
      "metadata": {
        "id": "9B4yhyeIjU9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 10: Calculation of output size and numbers of parameters\n",
        "def compute_conv_output_and_parameters(H_in, W_in, C_in, kernel_size, C_out, stride=1, padding=0):\n",
        "    kh, kw = kernel_size\n",
        "\n",
        "    # Output dimensions\n",
        "    H_out = (H_in + 2 * padding - kh) // stride + 1\n",
        "    W_out = (W_in + 2 * padding - kw) // stride + 1\n",
        "\n",
        "    # Parameters per filter: C_in * kh * kw, plus 1 bias per output channel\n",
        "    params_per_filter = C_in * kh * kw + 1\n",
        "    total_params = params_per_filter * C_out\n",
        "\n",
        "    return (H_out, W_out, C_out), total_params\n",
        "\n",
        "# 1. Input: 144x144x3, Filter: 3x3, 6 filters, stride=1, padding=0\n",
        "out1, params1 = compute_conv_output_and_parameters(144, 144, 3, (3, 3), 6)\n",
        "print(\"Layer 1 Output: \", out1, \"Params: \", params1)\n",
        "\n",
        "# 2. Input: 60x60x24, Filter: 3x3, 48 filters, stride=1, padding=0\n",
        "out2, params2 = compute_conv_output_and_parameters(60, 60, 24, (3, 3), 48)\n",
        "print(\"Layer 1 Output: \", out2, \"Params: \", params2)\n",
        "\n",
        "# 3. Input: 20x20x10, Filter: 3x3, 20 filters, stride=2, padding=0\n",
        "out3, params3 = compute_conv_output_and_parameters(20, 20, 10, (3, 3), 20, stride=2)\n",
        "print(\"Layer 1 Output: \", out3, \"Params: \", params3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQd6ea4ZRPF3",
        "outputId": "1dea6511-0d43-429e-f4c8-62315eb95688"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Output:  (142, 142, 6) Params:  168\n",
            "Layer 1 Output:  (58, 58, 48) Params:  10416\n",
            "Layer 1 Output:  (9, 9, 20) Params:  1820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###【Problem 11】(Advanced assignment) Survey on filter size"
      ],
      "metadata": {
        "id": "ryC28LSQWxhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why 3x3 filters are commonly used instead of larger ones such as 7x7**\n",
        "\n",
        "The use smaller-sized convolutional filters, e.g., 3x3 rather than large convolutional filters, e.g., 7x7, is a common convention of the  recent CNN architectures due tomultiple reasons. First, residual stacking with several layers composed of 3x3 is more non-linear to the network. The activation functions occur after each layer, thus with three 3x3 layers more activation operations can be performed than with one 7x7 layer, and in this away deeper and expressive networks are possible. Secondly, 3x3 filters have better usage of parameters. As an example, that 7x7 convolutional layer with input and output channels, C would have 49C 2 parameters, but with three convolutional layers of three pixels in size stacked on top of each other, we need only 27C 2 parameters a little more tanh half. The three 3x3 layers were keeping only 7x7 fields of influence despite the reduced parameter number effectively, the network would be able to comprehend the same spatial information with the benefit of marginally more learning capabilities and less computations.\n",
        "\n",
        "**The effect of a 1x1 filter with no height or width direction**\n",
        "\n",
        "This convolution filter fails to extract spatial information because height has a value of one and also the width. Rather, it acts on the channel dimension instead and has the effect of instantaneously performing a fully conected layer on every pixel position in the image. Primary applications of 1x1 filters are: reducing dimensionality of the input, e.g. reducing the number of channels prior to undergoing a more expensive computation-wise convolution and embedding the network with additional depth whitout expanding it spatially. Further, learning non-linear combination in the feature channels can be done using 1 by 1 convolutions. The idea was conceived in the Network-in-Network (NIN) architecture and it has been successfully implemented by GoogLeNet (Inception), in which it was instrumental to reduce the depth of the networks and improve efficiency."
      ],
      "metadata": {
        "id": "aeNcK_7IW6Xh"
      }
    }
  ]
}